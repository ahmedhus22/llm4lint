{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Training Encoder-Decoder Transformer \n",
    "Training is performed in 3 steps based on [GPT](https://openai.com/index/instruction-following/)\n",
    "- Pretraining\n",
    "- Fine-tuning\n",
    "- Ranking and RLHF (Ignored for now)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pathlib import Path\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import math\n",
    "from copy import deepcopy\n",
    "\n",
    "from tqdm.notebook import trange, tqdm\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch import optim\n",
    "from torch.utils.data import DataLoader\n",
    "from torch.utils.data.dataset import Dataset\n",
    "import torch.nn.functional as F\n",
    "from torch.distributions import Categorical\n",
    "\n",
    "import datasets\n",
    "from transformers import AutoTokenizer\n",
    "from tokenizers import Tokenizer\n",
    "from tokenizers.trainers import BpeTrainer\n",
    "from tokenizers.pre_tokenizers import ByteLevel\n",
    "from tokenizers.processors import TemplateProcessing\n",
    "from tokenizers.models import BPE\n",
    "\n",
    "torch.backends.cuda.matmul.allow_tf32=True"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define the learning rate for the optimizer\n",
    "learning_rate = 1e-4\n",
    "\n",
    "# Define the number of epochs for training\n",
    "nepochs = 20\n",
    "\n",
    "# Define the batch size for mini-batch gradient descent\n",
    "batch_size = 2\n",
    "\n",
    "# Define the root directory of the dataset\n",
    "DATASET_PATH = Path(\"../datasets\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Dataprocessing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class CodeLintDataset(Dataset):\n",
    "    def __init__(self, dataset_path):\n",
    "        self.df = pd.read_csv(dataset_path)\n",
    "        # self.df.fillna('', inplace=True)\n",
    "    \n",
    "    def __getitem__(self, index):\n",
    "        return self.df.loc[index][\"code\"], self.df.loc[index][\"label\"]\n",
    "    \n",
    "    def __len__(self):\n",
    "        return len(self.df)\n",
    "    \n",
    "stackv2_python = CodeLintDataset(DATASET_PATH / Path(\"dataset_pylint.csv\"))\n",
    "data_loader_train = DataLoader(stackv2_python, batch_size=batch_size, shuffle=True)\n",
    "#data_loader_test = DataLoader(dataset_test, batch_size=batch_size, shuffle=True, num_workers=4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "next(iter(data_loader_train))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Training Tokenizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# gpt2_tokenizer = AutoTokenizer.from_pretrained(\"gpt2\")\n",
    "# def get_training_corpus():\n",
    "#     dataset = datasets.load_dataset(\"csv\", data_files=str(DATASET_PATH / Path(\"dataset_pylint.csv\")))[\"train\"][\"code\"]\n",
    "#     for start_idx in range(0, len(dataset), 1000):\n",
    "#         samples = dataset[start_idx : start_idx + 1000]\n",
    "#         yield samples\n",
    "# training_corpus = get_training_corpus()\n",
    "# gpt2_tokenizer.add_special_tokens({\"additional_special_tokens\":[\"[SOC]\", \"[EOC]\"]})\n",
    "# print(gpt2_tokenizer.all_special_tokens)\n",
    "# python_tokenizer = gpt2_tokenizer.train_new_from_iterator(training_corpus, 52000)\n",
    "\n",
    "# # python_tokenizer.post_processor = TemplateProcessing(\n",
    "# #     single=\"[SOC] $A [EOC]\",\n",
    "# #     pair=\"[SOC] $A [SEP] $B:1 [EOC]\",\n",
    "# #     special_tokens=[\n",
    "# #         (\"[SOC]\", 0),\n",
    "# #         (\"[SEP]\", 1),\n",
    "# #         (\"[EOC]\", 2)\n",
    "# #     ]\n",
    "# # )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# example_enc = python_tokenizer.encode('''def add_numbers(a, b):\n",
    "#     \"\"\"Add the two numbers `a` and `b`.\"\"\"\n",
    "#     return a + b''')\n",
    "# #python_tokenizer.decode(example_enc)\n",
    "# example_enc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# bpe_tokenizer = Tokenizer(BPE(unk_token=\"[UNK]\"))\n",
    "# trainer = BpeTrainer(special_tokens=[\"[UNK]\", \"[SOC]\", \"[SEP]\", \"[EOC]\", \"[PAD]\", \"[MASK]\"],  min_frequency=2)\n",
    "# #files = [str(DATASET_PATH / Path(\"code_raw.csv\"))] # USE ITERATOR INSTEAD!!!\n",
    "# bpe_tokenizer.train_from_iterator(stackv2_python.df[\"code\"], trainer)\n",
    "# bpe_tokenizer.post_processor = TemplateProcessing(\n",
    "#     single=\"[SOC] $A [EOC]\",\n",
    "#     pair=\"[SOC] $A [SEP] $B:1 [EOC]:1\",\n",
    "#     special_tokens=[\n",
    "#         (\"[SOC]\", bpe_tokenizer.token_to_id(\"[SOC]\")),\n",
    "#         (\"[SEP]\", bpe_tokenizer.token_to_id(\"[SEP]\")),\n",
    "#         (\"[EOC]\", bpe_tokenizer.token_to_id(\"[EOC]\")),\n",
    "#     ],\n",
    "# )\n",
    "# bpe_tokenizer.enable_padding(pad_id=bpe_tokenizer.token_to_id(\"[PAD]\"), pad_token=\"[PAD]\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# EXPERIMENTING A DIFFERENT TOKENIZER"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "bpe_tokenizer = Tokenizer(BPE(unk_token=\"[UNK]\"))\n",
    "trainer = BpeTrainer(special_tokens=[\"[PAD]\", \"[UNK]\", \"[MASK]\", \"[SOC]\", \"[SEP]\", \"[EOC]\", \"[SOL]\", \"[EOL]\"],  min_frequency=2)\n",
    "#files = [str(DATASET_PATH / Path(\"code_raw.csv\"))] # USE ITERATOR INSTEAD!!!\n",
    "bpe_tokenizer.train_from_iterator(stackv2_python.df[\"code\"], trainer) # USE datasets TO CREATE BATCHES TO SPEED UP! (training from memory doc)\n",
    "bpe_tokenizer.post_processor = TemplateProcessing(\n",
    "    single=\"[SOC] $A [EOC]\",\n",
    "    pair=\"[SOC] $A [SEP] $B:1 [EOC]:1\",\n",
    "    special_tokens=[\n",
    "        (\"[SOC]\", bpe_tokenizer.token_to_id(\"[SOC]\")),\n",
    "        (\"[SEP]\", bpe_tokenizer.token_to_id(\"[SEP]\")),\n",
    "        (\"[EOC]\", bpe_tokenizer.token_to_id(\"[EOC]\")),\n",
    "    ],\n",
    ")\n",
    "bpe_tokenizer.enable_padding(pad_id=bpe_tokenizer.token_to_id(\"[PAD]\"), pad_token=\"[PAD]\")\n",
    "#bpe_tokenizer.enable_truncation()\n",
    "bpe_tokenizer.save(\"bpe_tokenizer.json\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "bpe_tokenizer = Tokenizer.from_file(\"bpe_tokenizer.json\")\n",
    "bpe_tokenizer.enable_truncation(2000)\n",
    "label_bpe_tokenizer = deepcopy(bpe_tokenizer)\n",
    "label_bpe_tokenizer.post_processor = TemplateProcessing(\n",
    "    single=\"[SOL] $A [EOL]\",\n",
    "    pair=\"[SOL] $A [SEP] $B:1 [EOL]:1\",\n",
    "    special_tokens=[\n",
    "        (\"[SOL]\", bpe_tokenizer.token_to_id(\"[SOL]\")),\n",
    "        (\"[SEP]\", bpe_tokenizer.token_to_id(\"[SEP]\")),\n",
    "        (\"[EOL]\", bpe_tokenizer.token_to_id(\"[EOL]\")),\n",
    "    ],\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#files = [str(DATASET_PATH / Path(\"code_raw.csv\"))] # USE ITERATOR INSTEAD!!!\n",
    "# dataset = datasets.load_dataset(\"csv\", data_files=str(DATASET_PATH / Path(\"dataset_pylint.csv\")))\n",
    "# def batch_iterator(batch_size=1000):\n",
    "#     # Only keep the text column to avoid decoding the rest of the columns unnecessarily\n",
    "#     tok_dataset = dataset.select_columns(\"code\")\n",
    "#     for batch in tok_dataset.iter(batch_size):\n",
    "#         yield batch[\"code\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "example = '''def linear(args, output_size, bias, bias_start=0.0, scope=None, squeeze=False, wd=0.0, input_keep_prob=1.0,\n",
    "           is_train=None):#, name_w='', name_b=''\n",
    "    # if args is None or (nest.is_sequence(args) and not args):\n",
    "    #     raise ValueError(\"\"`args` must be specified\"\")\n",
    "    # if not nest.is_sequence(args):\n",
    "    #     args = [args]'''\n",
    "bpe_tokenizer.encode(example).tokens"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "label_bpe_tokenizer.encode(example).tokens"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Model Architecture"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# sinusoidal positional embeds\n",
    "class SinusoidalPosEmb(nn.Module):\n",
    "    def __init__(self, dim):\n",
    "        super().__init__()\n",
    "        self.dim = dim\n",
    "\n",
    "    def forward(self, x):\n",
    "        device = x.device\n",
    "        half_dim = self.dim // 2\n",
    "        emb = math.log(10000) / (half_dim - 1)\n",
    "        emb = torch.exp(torch.arange(half_dim, device=device) * -emb)\n",
    "        emb = x[:, None] * emb[None, :]\n",
    "        emb = torch.cat((emb.sin(), emb.cos()), dim=-1)\n",
    "        return emb\n",
    "\n",
    "\n",
    "# Define a module for attention blocks\n",
    "class AttentionBlock(nn.Module):\n",
    "    def __init__(self, hidden_size=128, num_heads=4, masking=True):\n",
    "        super(AttentionBlock, self).__init__()\n",
    "        self.masking = masking\n",
    "\n",
    "        # Multi-head attention mechanism\n",
    "        self.multihead_attn = nn.MultiheadAttention(hidden_size,\n",
    "                                                    num_heads=num_heads,\n",
    "                                                    batch_first=True,\n",
    "                                                    dropout=0.25)\n",
    "\n",
    "    def forward(self, x_in, kv_in, key_mask=None):\n",
    "        # Apply causal masking if enabled\n",
    "        if self.masking:\n",
    "            bs, l, h = x_in.shape\n",
    "            mask = torch.triu(torch.ones(l, l, device=x_in.device), 1).bool()\n",
    "        else:\n",
    "            mask = None\n",
    "            \n",
    "        # Perform multi-head attention operation\n",
    "        return self.multihead_attn(x_in, kv_in, kv_in, attn_mask=mask, key_padding_mask=key_mask)[0]\n",
    "\n",
    "\n",
    "# Define a module for a transformer block with self-attention and optional causal masking\n",
    "class TransformerBlock(nn.Module):\n",
    "    def __init__(self, hidden_size=128, num_heads=4, is_decoder=False, masking=True):\n",
    "        super(TransformerBlock, self).__init__()\n",
    "        self.is_decoder = is_decoder\n",
    "\n",
    "        # Layer normalization for the input\n",
    "        self.norm1 = nn.LayerNorm(hidden_size)\n",
    "        # Self-attention mechanism\n",
    "        self.attn1 = AttentionBlock(hidden_size=hidden_size, num_heads=num_heads, masking=masking)\n",
    "        \n",
    "        # Layer normalization for the output of the first attention layer\n",
    "        if self.is_decoder:\n",
    "            self.norm2 = nn.LayerNorm(hidden_size)\n",
    "            # Self-attention mechanism for the decoder with no masking\n",
    "            self.attn2 = AttentionBlock(hidden_size=hidden_size, num_heads=num_heads, masking=False)\n",
    "        \n",
    "        # Layer normalization for the output before the MLP\n",
    "        self.norm_mlp = nn.LayerNorm(hidden_size)\n",
    "        # Multi-layer perceptron (MLP)\n",
    "        self.mlp = nn.Sequential(nn.Linear(hidden_size, hidden_size * 4),\n",
    "                                 nn.ELU(),\n",
    "                                 nn.Linear(hidden_size * 4, hidden_size))\n",
    "                \n",
    "    def forward(self, x, input_key_mask=None, cross_key_mask=None, kv_cross=None):\n",
    "        # Perform self-attention operation\n",
    "        x = self.attn1(x, x, key_mask=input_key_mask) + x\n",
    "        x = self.norm1(x)\n",
    "\n",
    "        # If decoder, perform additional cross-attention layer\n",
    "        if self.is_decoder:\n",
    "            x = self.attn2(x, kv_cross, key_mask=cross_key_mask) + x\n",
    "            x = self.norm2(x)\n",
    "\n",
    "        # Apply MLP and layer normalization\n",
    "        x = self.mlp(x) + x\n",
    "        return self.norm_mlp(x)\n",
    "    \n",
    "    \n",
    "# Define an encoder module for the Transformer architecture\n",
    "class Encoder(nn.Module):\n",
    "    def __init__(self, num_emb, hidden_size=128, num_layers=3, num_heads=4):\n",
    "        super(Encoder, self).__init__()\n",
    "        \n",
    "        # Create an embedding layer for tokens\n",
    "        self.embedding = nn.Embedding(num_emb, hidden_size)\n",
    "        # Initialize the embedding weights\n",
    "        self.embedding.weight.data = 0.001 * self.embedding.weight.data\n",
    "\n",
    "        # Initialize sinusoidal positional embeddings\n",
    "        self.pos_emb = SinusoidalPosEmb(hidden_size)\n",
    "        \n",
    "        # Create multiple transformer blocks as layers\n",
    "        self.blocks = nn.ModuleList([\n",
    "            TransformerBlock(hidden_size, num_heads, is_decoder=False, masking=False) for _ in range(num_layers)\n",
    "        ])\n",
    "                \n",
    "    def forward(self, input_seq, padding_mask=None):        \n",
    "        # Embed the input sequence\n",
    "        input_embs = self.embedding(input_seq)\n",
    "        bs, l, h = input_embs.shape\n",
    "\n",
    "        # Add positional embeddings to the input embeddings\n",
    "        seq_indx = torch.arange(l, device=input_seq.device)\n",
    "        pos_emb = self.pos_emb(seq_indx).reshape(1, l, h).expand(bs, l, h)\n",
    "        embs = input_embs + pos_emb\n",
    "        \n",
    "        # Pass the embeddings through each transformer block\n",
    "        for block in self.blocks:\n",
    "            embs = block(embs, input_key_mask=padding_mask)\n",
    "        \n",
    "        return embs\n",
    "\n",
    "    \n",
    "# Define a decoder module for the Transformer architecture\n",
    "class Decoder(nn.Module):\n",
    "    def __init__(self, num_emb, hidden_size=128, num_layers=3, num_heads=4):\n",
    "        super(Decoder, self).__init__()\n",
    "        \n",
    "        # Create an embedding layer for tokens\n",
    "        self.embedding = nn.Embedding(num_emb, hidden_size)\n",
    "        # Initialize the embedding weights\n",
    "        self.embedding.weight.data = 0.001 * self.embedding.weight.data\n",
    "\n",
    "        # Initialize sinusoidal positional embeddings\n",
    "        self.pos_emb = SinusoidalPosEmb(hidden_size)\n",
    "        \n",
    "        # Create multiple transformer blocks as layers\n",
    "        self.blocks = nn.ModuleList([\n",
    "            TransformerBlock(hidden_size, num_heads, is_decoder=True, masking=True) for _ in range(num_layers)\n",
    "        ])\n",
    "                \n",
    "        # Define a linear layer for output prediction\n",
    "        self.fc_out = nn.Linear(hidden_size, num_emb)\n",
    "        \n",
    "    def forward(self, input_seq, encoder_output, input_padding_mask=None, encoder_padding_mask=None):        \n",
    "        # Embed the input sequence\n",
    "        input_embs = self.embedding(input_seq)\n",
    "        bs, l, h = input_embs.shape\n",
    "\n",
    "        # Add positional embeddings to the input embeddings\n",
    "        seq_indx = torch.arange(l, device=input_seq.device)\n",
    "        pos_emb = self.pos_emb(seq_indx).reshape(1, l, h).expand(bs, l, h)\n",
    "        embs = input_embs + pos_emb\n",
    "        \n",
    "        # Pass the embeddings through each transformer block\n",
    "        for block in self.blocks:\n",
    "            embs = block(embs,\n",
    "                         input_key_mask=input_padding_mask,\n",
    "                         cross_key_mask=encoder_padding_mask, \n",
    "                         kv_cross=encoder_output)\n",
    "        \n",
    "        return self.fc_out(embs)\n",
    "\n",
    "    \n",
    "# Define an Encoder-Decoder module for the Transformer architecture\n",
    "class EncoderDecoder(nn.Module):\n",
    "    def __init__(self, num_emb, hidden_size=128, num_layers=(3, 3), num_heads=4):\n",
    "        super(EncoderDecoder, self).__init__()\n",
    "        \n",
    "        # Create an encoder and decoder with specified parameters\n",
    "        self.encoder = Encoder(num_emb=num_emb, hidden_size=hidden_size, \n",
    "                               num_layers=num_layers[0], num_heads=num_heads)\n",
    "        \n",
    "        self.decoder = Decoder(num_emb=num_emb, hidden_size=hidden_size, \n",
    "                               num_layers=num_layers[1], num_heads=num_heads)\n",
    "\n",
    "    def forward(self, input_seq, target_seq):\n",
    "        # Generate padding masks for input and target sequences\n",
    "        input_key_mask = input_seq == 0\n",
    "        output_key_mask = target_seq == 0\n",
    "\n",
    "        # Encode the input sequence\n",
    "        encoded_seq = self.encoder(input_seq=input_seq, \n",
    "                                   padding_mask=input_key_mask)\n",
    "        \n",
    "        # Decode the target sequence using the encoded sequence\n",
    "        decoded_seq = self.decoder(input_seq=target_seq, \n",
    "                                   encoder_output=encoded_seq, \n",
    "                                   input_padding_mask=output_key_mask, \n",
    "                                   encoder_padding_mask=input_key_mask)\n",
    "\n",
    "        return decoded_seq"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check if GPU is available, set device accordingly\n",
    "device = torch.device(0 if torch.cuda.is_available() else 'cpu')\n",
    "\n",
    "# Embedding Size\n",
    "hidden_size = 512\n",
    "\n",
    "# Number of Transformer blocks for the (Encoder, Decoder)\n",
    "num_layers = (4, 4)\n",
    "\n",
    "# MultiheadAttention Heads\n",
    "num_heads = 8\n",
    "\n",
    "# Create model\n",
    "tf_generator = EncoderDecoder(num_emb=bpe_tokenizer.get_vocab_size(), num_layers=num_layers, \n",
    "                              hidden_size=hidden_size, num_heads=num_heads).to(device)\n",
    "\n",
    "# Initialize the optimizer with above parameters\n",
    "optimizer = optim.Adam(tf_generator.parameters(), lr=learning_rate)\n",
    "\n",
    "# Scaler for mixed precision training\n",
    "scaler = torch.amp.GradScaler(\"cuda\")\n",
    "\n",
    "# Define the loss function\n",
    "loss_fn = nn.CrossEntropyLoss(reduction=\"none\")\n",
    "\n",
    "# Initialize the training loss logger\n",
    "training_loss_logger = []\n",
    "\n",
    "start_epoch = 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Load Checkpoint\n",
    "# cp = torch.load(\"qa_model.pt\")\n",
    "# tf_generator.load_state_dict(cp[\"model_state_dict\"])\n",
    "# optimizer.load_state_dict(cp[\"optimizer_state_dict\"])\n",
    "# training_loss_logger = cp[\"data_logger\"]\n",
    "# start_epoch = cp[\"epoch\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Let's see how many Parameters our Model has!\n",
    "num_model_params = 0\n",
    "for param in tf_generator.parameters():\n",
    "    num_model_params += param.flatten().shape[0]\n",
    "\n",
    "print(\"-This Model Has %d (Approximately %d Million) Parameters!\" % (num_model_params, num_model_params//1e6))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Iterate over epochs\n",
    "for epoch in trange(start_epoch, nepochs, leave=False, desc=\"Epoch\"):\n",
    "    # Set the model in training mode\n",
    "    tf_generator.train()\n",
    "    \n",
    "    # Iterate over the training data loader\n",
    "    for code, label in tqdm(data_loader_train, desc=\"Training\", leave=False):\n",
    "        # Convert question and answer text to tokens and move to device\n",
    "        code_tokens = torch.tensor([encoding.ids for encoding in bpe_tokenizer.encode_batch(list(code))]).to(device) # NEED TO BE TESTED\n",
    "        label_tokens = torch.tensor([encoding.ids for encoding in label_bpe_tokenizer.encode_batch(list(label))]).to(device)\n",
    "        label_input_text = label_tokens[:, 0:-1]\n",
    "        label_output_text = label_tokens[:, 1:]\n",
    "\n",
    "        # Forward pass\n",
    "        with torch.amp.autocast(\"cuda\"):\n",
    "            pred = tf_generator(code_tokens, label_input_text)\n",
    "\n",
    "            # Generate mask for output text\n",
    "            output_mask = (label_output_text != bpe_tokenizer.token_to_id(\"[PAD]\")).float() # NEED TO BE TESTED\n",
    "\n",
    "            # Compute the loss\n",
    "            loss = (loss_fn(pred.transpose(1, 2), label_output_text) * output_mask).sum()/output_mask.sum()\n",
    "\n",
    "        # Backpropagation\n",
    "        optimizer.zero_grad()\n",
    "        scaler.scale(loss).backward()\n",
    "        scaler.step(optimizer)\n",
    "        scaler.update()\n",
    "        \n",
    "        # Log the training loss\n",
    "        training_loss_logger.append(loss.item())\n",
    "    \n",
    "    # Quick save of the model every epoch\n",
    "    torch.save({'epoch': epoch + 1,\n",
    "                'data_logger': training_loss_logger,\n",
    "                'model_state_dict': tf_generator.state_dict(),\n",
    "                'optimizer_state_dict': optimizer.state_dict(),\n",
    "                 }, \"qa_model.pt\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "code_tokens.size()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "_ = plt.figure(figsize=(10, 5))\n",
    "_ = plt.plot(training_loss_logger[10:])\n",
    "_ = plt.title(\"Training Loss\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Testing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get a batch of question and answer text from the test data loader\n",
    "q_text, a_text = next(iter(data_loader_train))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Choose an index within the batch\n",
    "index = 0\n",
    "\n",
    "# Print the code at the chosen index\n",
    "print(\"Input Code:\")\n",
    "print(q_text[index])\n",
    "input_q_tokens = torch.tensor(bpe_tokenizer.encode(q_text[index]).ids).unsqueeze(0)\n",
    "\n",
    "# Print the original label text at the chosen index\n",
    "print(\"\\nOriginal Label:\")\n",
    "print(a_text[index])\n",
    "soa_token = label_bpe_tokenizer.token_to_id(\"[SOL]\") * torch.ones(1, 1).long()\n",
    "temp = 0.3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "log_tokens = [soa_token]\n",
    "tf_generator.eval()\n",
    "\n",
    "with torch.no_grad():\n",
    "    # Encode the input question tokens\n",
    "    encoded_seq = tf_generator.encoder(input_q_tokens.to(device))\n",
    "\n",
    "    # Generate the answer tokens\n",
    "    for i in range(100):\n",
    "        input_tokens = torch.cat(log_tokens, 1)\n",
    "        \n",
    "        # Decode the input tokens into the next predicted tokens\n",
    "        data_pred = tf_generator.decoder(input_tokens.to(device), encoded_seq)\n",
    "        \n",
    "        # Sample from the distribution of predicted probabilities\n",
    "        dist = Categorical(logits=data_pred[:, -1] / temp)\n",
    "        next_tokens = dist.sample().reshape(1, 1)\n",
    "        \n",
    "        # Append the next predicted token to the sequence\n",
    "        log_tokens.append(next_tokens.cpu())\n",
    "        \n",
    "        # Break the loop if the End-Of-Answer token is predicted\n",
    "        if next_tokens.item() == label_bpe_tokenizer.token_to_id(\"[EOL]\"):\n",
    "            break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Convert the list of token indices to a tensor\n",
    "pred_text = torch.cat(log_tokens, 1)\n",
    "\n",
    "# Convert the token indices to their corresponding strings using the vocabulary\n",
    "pred_text_strings = label_bpe_tokenizer.decode(pred_text[0].numpy())\n",
    "\n",
    "# Join the token strings to form the predicted text\n",
    "pred_text = \"\".join(pred_text_strings)\n",
    "\n",
    "# Print the predicted text\n",
    "print(pred_text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "llmlint",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.20"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
